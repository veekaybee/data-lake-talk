<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport">
	<title>It Came from the Data Lake</title>
	<link href="css/reveal.css" rel="stylesheet">
	<link href="css/theme/white.css" rel="stylesheet"><!-- Theme used for syntax highlighting of code -->
	<link href="lib/css/zenburn.css" rel="stylesheet"><!-- Printing and PDF exports -->

	<script>
	           var link = document.createElement( 'link' );
	           link.rel = 'stylesheet';
	           link.type = 'text/css';
	           link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
	           document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>
</head>
<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<h3>Replace Hadoop with Your Laptop</h2><span style="color:blue">The Case for Multiprocessing </span>
				<br>Vicki Boykis @vboykis
				<center>
					<img src="img/snakelephant.png">
				</center>
			</section>
			<section>
				<h2>About me</h2><span style="color:blue">Data Science + Engineering @ <a href="http://www.captechconsulting.com/">CapTech</a></span>
				<center>
					<br>
					<p align="center">GitHub: <a href="https://github.com/veekaybee">@veekaybee</a><br></p>
					<align left="">
						Twitter: <a href="http://twitter.com/vboykis">@vboykis</a><br>
						Website: <a href="http://www.vickiboykis.com">vickiboykis.com</a><br>
						<img align="center" src="img/stack.png">
					</align>
				</center>
			</section>
			<section>
				<h2>Agenda<br><img src="img/snake.png"><br></h2>
						<ul>
							<li>Your computer is good enough</li>
							<li>You don't always have big data</li>
							<li>Multiprocessing</li>
							<li>Distributed system overhead</li>
						</ul>

			</section>

			<section>
				<h4>"There is a time for every data environment"</h4>
				<h4>-The Byrds, Probably</h4>
				<img src="img/byrds.png" align="right">
						<blockquote>
							To everything (turn turn)<br>
							There is a season (turn turn)<br>
							And a time to every purpose<br>
							A time to laugh<br>
							A time to cry<br>
							A time for distributed systems<br>
							A time for local processing<br>
							A time for healing<br>
						</blockquote>
						

			</section>
				<section>
				<h2>My computing environment<br><img src="img/environment.png"><br></h2>

			</section>
			<section>
				<h3>You don't always have big data</h3><img src="img/bigdata.png">
				<br>And, how frequently are you processing these sizes of data? 
			</section>

			<section>
				<h3>How much data can your laptop process?</h3>
				<ul>
					<li style="list-style: none"><font size="+3"></font></li>
					
					<li><font size="+3"><a href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html">Databricks Tungsten:</a> "Aggregation and joins on one billion records on one machine in less than 1 second."</font></li>
					<li><font size="+3"><a href="https://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">Command Line:</a> 1.75GB containing around 2 million chess games. "This <code>find | xargs mawk | mawk</code> pipeline gets us down to a runtime of about 12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop implementation."</font></li>
				</ul><font size="+3"></font>
			</section>
			<section>
				<h3>Les MapReduces ‚Äì Word count exercise</h3><img src="img/lesmapreduces.png"> <a href="http://www.gutenberg.org/ebooks/135">Les Miserables</a> - 655k words / 3.2 MB textfile
			</section>

			
			<section>
				<h2>Checking out the file</h2>
				<pre>
                            <code class="bash" data-trim="">
                    vboykis$ head -n 1000  lesmiserables.txt

                    On the other hand, this affair afforded great delight to Madame Magloire. "Good," said she to Mademoiselle Baptistine; "Monseigneur began with other people, but he has had to wind up with himself, after all. He has regulated all his charities. Now here are three thousand francs for us! At last!"

                    That same evening the Bishop wrote out and handed to his sister a memorandum conceived in the following terms:--
                                </code>
                                    </pre>
			</section>
			<section>
				<h3>Command line MapReduce</h3>
				<pre>
                            <code class="bash" data-trim="">
                    sed -e 's/[^[:alpha:]]/ /g' lesmiserables.txt \ # only alpha 
                                    | tr '\n' " " \ # replace lines with spaces
                                    |  tr -s " "  \ # compresses adjacent spaces
                                    | tr " " '\n' \  # spaces to linebreaks
                                    | tr 'A-Z' 'a-z' \ # removes uppercase
                                    | sort \ # sorts words alphabetically
                                    | uniq -c \ # counts unique occurrences
                                    | sort -nr \ # sorts in numeric order reverse
                                    | nl \ # line numbers
                                46  1374 marius
                                47  1366 when
                                48  1316 we
                                49  1252 their
                                50  1238 jean


                                </code>
                            </pre>
                                   
			</section>

			<section> <h3>Let's say we have lots of files</h3>


					<pre>
				        <code class="bash" data-trim="">

						# Makes n number of copies of Les Miserables

						INPUT=lesmiserables.txt
						for num in $(seq 1 1000)
						do
						    bn=$(basename $INPUT .txt)
						    cp $INPUT $bn$num.txt
						done
				        </code> 

						<code class="bash" data-trim="">
						#Before
				 mbp-vboykis:data-lake-code vboykis$ du -sh 
						3.3M
						#After
						mbp-vboykis:data-lake-code vboykis$ du -sh 
						3.2G
						</code> 

						<h3>Let's say we have a large file.</h3>

				</section>

			<section>
				<h3>It's now easier to write Python</h3>
				This is the heart of the "algorithm"
				<div id="column1" style="float:left; width:50%;">
				<pre>
					
                            <code class="python" data-trim="">
					def map(words):
					    """Inserts each word into a dictionary
					    and counts them. 
					    :param words: List of words
					    """
						result = {}
						for i in words:
						    try:
						        result[i] += 1
						    except KeyError:
						        result[i] = 1
						return result   
                                </code></div> </pre>

						<div id="column2" style="float:left; width:50%;">
							<pre>
						<code class="python" data-trim="">
						def reduce(dict_list):
							"""Reduces a dictionary with mapped keys/values by summing values
						    	:param words: List of words
						    	"""
							d = {}
							for entry in dict_list:
								print("merging dictionary...")
								for k, v in entry.items():
									try:
										d[k] += v
									except KeyError:
										d[k] = v
							return d 
						</code></div></pre><br>

				<img  src="img/github.png"><a href="https://github.com/veekaybee/data-lake-code/blob/master/mapreduce.py">GitHub</a>
			</section>

			<section> 
				<h2>How long does it take? 
					<br>Killed after 13 minutes</h2>
						<pre>
						<code class="python" data-trim="">

mbp-vboykis:data-lake-code vboykis$ time python mapreduce.py 
Size of files: 6.195150626823306 GB
Processing textfiles...
[1]+  Stopped                 python3 mapreduce.py
				real	13m17.698s
				</code></pre>

				</section>

				<section> <h2>How do we speed it up?<h2> 

					üéâ Multiprocessing!üéâ 
					<br>(and üêçPyPyüêç)

					</section>

					<section> <h3>What are threads and processes? </h3> 

						<img src="img/gil.png">
						<br>And what's a GIL? 

				</section>


					<section> <h3>How Python Utilizes Hardware</h3> 

						<pre>
<code class="python" data-trim="">
vboykis$ python mapreduce.py
vboykis$ top
PID    COMMAND      %CPU  TIME     #TH      STATE
32057  Python       99.0  00:04.16 1/1    running
</code></pre>


				<img src="img/threadscores.png">
			</section>

			<section> <h2>Multiprocessing API</h2> 
				
				<div align="left">
				<p>
				<font size="6" align="left"><b>Process:</b> Each function call is a separate process (good for small programs)<br>
				<br><b>Pool:</b> Allows you to control number of processes, maintains order of returned results</font></p></div>
				<img src="img/processpool.png">

				</section>

			<section>

				
				<h3>Our Multiprocessing Code</h3>
				<pre>
                            <code class="python" data-trim="">

	files = get_filenames()
	pool= Pool(processes=5) 
	result_set = pool.map(map_function, files, chunksize=30) 
	show_items(reduce_function(result_set))

	PID    COMMAND      %CPU TIME     #TH    
	32984  Python       94.4 01:16.48 1/1    
	32983  Python       94.8 01:14.76 1/1   
	32982  Python       77.4 01:17.70 1/1   
	32981  Python       95.2 01:15.60 1/1   
	32980  Python       96.9 01:14.00 1/1    

                                </code>
                                    </pre><img align="middle" src="img/github.png"><a href="https://github.com/veekaybee/data-lake-code/blob/master/mapreduce.py">GitHub</a>
			</section>

	<section>
				<h3>Our Multiprocessing Code</h3>
				<pre>

					 <code class="python" data-trim="">

                    pool= Pool(processes=5)
                    result_set = pool.map(map_function, get_filenames(), chunksize=30)

                    mbp-vboykis:data_lake vboykis$ time mapreduce.py
                    ('Size of files:', 3.099123327061534, 'GB')
                    
                    real	2m13.120s  user	8m47.443ssys	0m24.635s

                    mbp-vboykis:data_lake vboykis$ time pypy mapreduce.py

                    "‚ÄúIf you want your code to run faster, you should probably just use PyPy.‚Äù ‚Äî Guido"

                    real	2m10.799s  user	6m28.577s  sys	0m42.156s
                    ('Size of files:', 3.099123327061534, 'GB')

                    mbp-vboykis:data_lake vboykis$ time pypy mapreduce.py
                    ('Size of files:', 6.195150626823306, 'GB')

                    real    4m23.917suser 12m6.691ssys  1m39.279s

                    Valjean' - 1552776
                    'out' - 1580790
                    'little' - 1682841
                    'its' - 1726863
                    'than' - 1728864
                    'like' - 1804902
                    'very' - 1806903
                    'or' - 1812906
                    'A' - 1812906
                    'Marius' - 1812906

                                </code>
                                    </pre>

</section>




			<section>
				<h3>Recap</h3>

				<ul>
					<li> You can process fairly large amounts of data(5-10 GB) locally if you have a modern dev machine
					<li> Use Multiprocessing if your code is amenable to it
					<li> Structure your code for multiprocessing
					<li> Try different Pool/Chunksize combinations to tweak
					<li> PyPy!
					</ul> 

			</section>


                               
			</section>

			<section>
				<h3>The cost of a system</h3>
				<img src="img/blowfish.png">
				<br><img align="left" src="img/cost.png"> In some cases, an optimized single node is better than unoptimized multiple nodes. <a href="https://pdfs.semanticscholar.org/6753/959eed800e9fad9e330daae43f81b7a48017.pdf"><br>
				Source</a>
			</section>


			<section>


				<h4>MapReduce with Spark locally: <br>less code, more overhead</h4>

				<div id="column1" style="float:left; width:50%;">
				<img src="img/threads.png"> 
				<a href="http://www.python-course.eu/threads.php"><br>
				<font size="-1">source</font></a> </div>

				<div id="column2" style="float:left; width:50%;">
				<img src="img/spark-local-architecture.png"> 
				<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html"><font size="-1">source</font></a></div>
				

				<pre>
                            <code class="python" data-trim=""> mbp-vboykis:data_lake vboykis$ time ./bin/spark-submit --master local[5]/ spark_wordcount.py
                            </code>
                                    </pre>
			</section>
			<section>
				<h4>MapReduce with Spark</h4>
				<pre>
                            <code class="python" data-trim=""> 
                           

                    sc = SparkContext("local", "Les Mis Word Count") 

                    logFile = "/lesmiserables*.txt"

                    wordcounts = sc.textFile(logFile).map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \
                            .flatMap(lambda x: x.split()) \
                            .map(lambda x: (x, 1)) \
                            .reduceByKey(lambda x,y:x+y) \
                            .map(lambda x:(x[1],x[0])) \
                            .sortByKey(False) 
                    
                    print(wordcounts.take(10)) #print first 10 results 
                    sc.stop()
                                </code>
                                    </pre><img align="middle" src="img/github.png"><a href="https://github.com/veekaybee/data-lake-code/blob/master/spark_wordcount.py">GitHub</a>
			</section>

			<section>
				
				<h4>What big data can't give you</h4><img src="img/message-in-a-bottle.png"><br>
				<ul>
					<li>Data integrity
						<ul>
							<li>(denormalized, naming conventions)</li>
						</ul>
					</li>
					<li>SQL Query analyst speed</li>
					<li>Traditional data guarantees (consistency)</li>
					<li>Not a transactional database</li>
				</ul>

			</section>
			
			<section>
				<h4>What big data can give you</h4><img src="img/whale-breathing.png">
				<ul>
					<li>Storage of unstructured data</li>
					<li>Fault tolerance and availability</li>
					<li>A centralized place across departments</li>
					<li>Extremely heavy parallelized usage</li>
					<li>Ability to programmatically work with data</li>
				</ul>
			</section>
			<section>
				<h3>Good use cases for Hadoop</h3><img src="img/elephant.png">
				<ul>
					<li> A lot of data (more than 1 TB and growing)
					<li>Unstructured data (images, video, <a href="https://twitter.com/denormalize/status/832442855683616768">metadata</a>) </li>
					<li>Streaming data that needs to be stored quickly (logs) </li>
					<li>Many researchers need access in parallel</li>
					<li>You need to access and analyze ALL THE DATA </li>
					<li>You have a dedicated team of people to work on it 
						(@ least 1-2 dev, 1 admin, 1 analyst)</li><br>

				</ul>
			</section>
			<section>
				<h3>Good use cases for Hadoop</h3>
				<div id="column1" style="float:left; width:50%;">
					<a href="http://techblog.netflix.com/2014/10/using-presto-in-our-big-data-platform.html">Netflix</a><br>


					
					<ul>
						<li>10 PB warehouse.
						<li>2500 queries a day.<br></li>
						<li>10 million -&gt; 80 million members<br></li>
						<li>Thousands of devices, each with their own app</li>
						<li>Need to analyze user intent, uptime</li>
					</ul></a></font>
				</div>
				<div id="column2" style="float:right; width:50%;">
					<a href="http://www.sdss.org/">Sloan Digital Sky Survey</a> 

					    
					    <ul><li>Most detailed three-dimensional maps of the universe
						<li>250 mil stars, 208 mil galaxies<br></li>
						<li>Conduct sky measurement, stitch together sky maps</li></ul>
						</font><br>
				</div>
			</section>
			<section>
				<h2>Hadoop Optimizations</h2><img src="img/fishing.png">
				<ul>
					<li>Admin Optimization</li>
						<ul>
						<li>Naming Conventions (HDFS - filesystem)</li>
						<li>Security (Authentication, Authorization, OS)</li>
						<li>Team size</li>
					</ul>	
				  <li>Hadoop Internals</li>
				  <ul>
					<li>File Formats</li>
					<li>Language Optimization</li></ul>
				</ul>
					
				</ul>
			</section>
			<section>
				<h2>Hadoop File Formats</h2>
				<ul><font size="5">
					<li>File Compression</li>
<table>
<table class="tg">
  <tr>
    <th class="tg-yw4l">Compression format</th>
    <th class="tg-yw4l">Tool</th>
    <th class="tg-yw4l">Algorithm</th>
    <th class="tg-yw4l">File extention</th>
    <th class="tg-yw4l">Splittable</th>
  </tr>
  <tr>
    <td class="tg-yw4l">gzip</td>
    <td class="tg-yw4l">gzip</td>
    <td class="tg-yw4l">DEFLATE</td>
    <td class="tg-yw4l">.gz</td>
    <td class="tg-yw4l">No</td>
  </tr>
  <tr>
    <td class="tg-yw4l">bzip2</td>
    <td class="tg-yw4l">bizp2</td>
    <td class="tg-yw4l">bzip2</td>
    <td class="tg-yw4l">.bz2</td>
    <td class="tg-yw4l">Yes</td>
  </tr>
  <tr>
    <td class="tg-yw4l">LZO</td>
    <td class="tg-yw4l">lzop</td>
    <td class="tg-yw4l">LZO</td>
    <td class="tg-yw4l">.lzo</td>
    <td class="tg-yw4l">Yes if indexed</td>
  </tr>
  <tr>
    <td class="tg-yw4l">Snappy</td>
    <td class="tg-yw4l">N/A</td>
    <td class="tg-yw4l">Snappy</td>
    <td class="tg-yw4l">.snappy</td>
    <td class="tg-yw4l">No</td>
  </tr>
</table>
					
					<li>Avro - write-heavy</li>
					<ul>
						<li> Row-based storage format
						<li> Contains its own schema and schema evolution
						<li> Amenable to "full-table scans"
						</ul>
					<li>Parquet - read-heavy</li>
					<ul>
						<li> Columnar storage formats, sometimes used for final storage
							<li> Smaller disk-reads
						<li> Great for feature selection </ul>
					<li>ORC - read-heavy</li>
						<ul>
							<li> Defaults to Zlib compression
								<li> Mixed row-column, splittable
						</ul>
				</ul>
			</font>
			</section>

			<section>
				<h3><a href="http://www.slideshare.net/databricks/spark-summit-east-2017-matei-zaharia-keynote-trends-for-big-data-and-apache-spark-in-2017">Spark Development Languages</a></h3><img src="img/spark-languages.png">
				<ul>
					<li><a href= "https://databricks.com/blog/2014/04/14/spark-with-java-8.html">Java 8</a> - Lambda Expressions</li>
					<li><a href="https://www.quora.com/Why-is-the-Scala-compiler-so-slow">Scala</a> - Compile Time</li>
					<li>Python - Adoption Rate</li>
				</ul>
			</section>
			<section>
				<h2>Resources</h2>

				<div align="center">

				Icons: made by <a href="http://www.freepik.com" title="Freepik">Freepik</a> <br>from <a href=http://www.flaticon.com" title="Flaticon">www.flaticon.com</a> <br>are licensed by <a href="http://creativecommons.org/licenses/by/3.0/"title="Creative Commons BY 3.0">CC 3.0 </a><br>


				<ul>Links:
					<li><a href= "https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">Don't use Hadoop - your data isn't that big</a>
					</li>
					<li><a href= "https://github.com/aphyr/distsys-class/blob/master/README.markdown">An introduction to distributed systems</a>
						<li><a href= "https://www.slideshare.net/databricks/spark-summit-east-2017-matei-zaharia-keynote-trends-for-big-data-and-apache-spark-in-2017">Spark Trends 2017</a>
							<li><a href= "https://www.quora.com/What-is-the-best-directory-structure-to-store-different-types-of-logs-in-HDFS-over-the-time">Directory Structure for HDFS</a>

							

					</li>
				</ul>
			</div>

			

			</section>
		</div>
	</div>
	<script src="lib/js/head.min.js">
	</script> 
	<script src="js/reveal.js">
	</script> 
	<script>
	           // More info https://github.com/hakimel/reveal.js#configuration
	           Reveal.initialize({
	               history: true,

	               transition: 'none',

    				slideNumber: true,

	               // More info https://github.com/hakimel/reveal.js#dependencies
	               dependencies: [
	                   { src: 'plugin/markdown/marked.js' },
	                   { src: 'plugin/markdown/markdown.js' },
	                   { src: 'plugin/notes/notes.js', async: true },
	                   { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
	               ]
	           });
	</script>
</body>
</html>