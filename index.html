<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport">
    <title>It Came from the Data Lake</title>
    <link href="css/reveal.css" rel="stylesheet">
    <link href="css/theme/white.css" rel="stylesheet"><!-- Theme used for syntax highlighting of code -->
    <link href="lib/css/zenburn.css" rel="stylesheet"><!-- Printing and PDF exports -->

    <script>
               var link = document.createElement( 'link' );
               link.rel = 'stylesheet';
               link.type = 'text/css';
               link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
               document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <h3>Replace Hadoop with Your Laptop</h2><span style="color:blue"></span>
                	<h3> üêçMultiprocessingüêç</h3>
                Vicki Boykis | @vboykis

                <center>
                    <img src="img/snakelephant.png">
                </center>
            </section>
            <section>
                <h2>About me</h2><span style="color:blue">Data Science + Engineering @ <a href="http://www.captechconsulting.com/">CapTech</a></span>
                <center>
                    <br>
                    <p align="center">GitHub: <a href="https://github.com/veekaybee">veekaybee</a><br></p>
                    <align left="">
                        Twitter: <a href="http://twitter.com/vboykis">@vboykis</a><br>
                        Website: <a href="http://www.vickiboykis.com">vickiboykis.com</a><br>
                        <img align="center" src="img/stack.png">
                    </align>
                </center>
            </section>
            <section>
                <h2>Agenda<br><img src="img/snake.png"><br></h2>
                        <ul>
                            <li>Your computer is good enough</li>
                            <li>You don't always have big data</li>
                            <li>Multiprocessing makes it faster</li>
                            <li>When you need distributed systems</li>
                        </ul>

            </section>

            <section>
                <h4>"There is a time for every data environment"</h4>
                <h4>-The Byrds, Probably</h4>
                <img src="img/byrds.png" align="right">
                        <quote><align ="left">
                            To everything (turn turn)<br>
                            There is a season (turn turn)<br>
                            And a time to every purpose<br>
                            A time to laugh<br>
                            A time to cry<br>
                            A time for distributed systems<br>
                            A time for local processing<br>
                            A time for healing<br>
                        </quote></align>
                        

            </section>
                <section>
                <h2>My local environment<br></h2>
                	<img src="img/environment.png">
                <img src="img/threadscores.png">




            </section>


              <section>
                <h3>Boss: "Please analyze Les MapReduces for me"</h3>
<a href="http://www.gutenberg.org/ebooks/135">Les Miserables</a> - 655k words / 3.2 MB textfile
                <img src="img/lesmapreduces.png"> 
            </section>
            <section>
                <h3>You don't always have big data</h3><img src="img/bigdata.png">
            </section>

            <section>
                <h3>How much data can your laptop process?</h3>
                <ul>
                    <li style="list-style: none"><font size="+3"></font></li>
                    
                    <li><font size="+3"><a href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html">Databricks Tungsten:</a> "Aggregation and joins on one billion records on one machine in less than 1 second." (2013 Macbook Pro)</font></li>
                    <li><font size="+3"><a href="https://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">Command Line:</a> 1.75GB containing around 2 million chess games. "This <code>find | xargs mawk | mawk</code> pipeline gets us down to a runtime of about 12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop implementation."</font></li>
                </ul><font size="+3"></font>
            </section>
          

            
            <section>
                <h2>Checking out the file</h2>
                <pre>
                            <code class="bash" data-trim="">
                    vboykis$ head -n 1000  lesmiserables.txt

                    On the other hand, this affair afforded great delight to Madame Magloire. "Good," said she to Mademoiselle Baptistine; "Monseigneur began with other people, but he has had to wind up with himself, after all. He has regulated all his charities. Now here are three thousand francs for us! At last!"

                    That same evening the Bishop wrote out and handed to his sister a memorandum conceived in the following terms:--
                                </code>
                                    </pre>
            </section>
            <section>
                <h3>Command line MapReduce</h3>
                <pre>
                            <code class="bash" data-trim="">
                    sed -e 's/[^[:alpha:]]/ /g' lesmiserables.txt \ # only alpha 
                                    | tr '\n' " " \ # replace lines with spaces
                                    |  tr -s " "  \ # compresses adjacent spaces
                                    | tr " " '\n' \  # spaces to linebreaks
                                    | tr 'A-Z' 'a-z' \ # removes uppercase
                                    | sort \ # sorts words alphabetically
                                    | uniq -c \ # counts unique occurrences
                                    | sort -nr \ # sorts in numeric order reverse
                                    | nl \ # line numbers
                                46  1374 marius
                                47  1366 when
                                48  1316 we
                                49  1252 their
                                50  1238 jean


                                </code>
                            </pre>
                                   
            </section>

            <section> <h3>Let's say we have lots of files</h3>


                    <pre>
                        <code class="bash" data-trim="">

                        # Makes n number of copies of Les Miserables

                        INPUT=lesmiserables.txt
                        for num in $(seq 1 1000)
                        do
                            bn=$(basename $INPUT .txt)
                            cp $INPUT $bn$num.txt
                        done
                        </code> 

                        <code class="bash" data-trim="">
                        #Before
                        vboykis$ du -sh 
                        3.3M
                        #After
                        vboykis$ du -sh 
                        3.2G
                        </code> 

                        

                </section>

            <section>
                <h3>We should write some Python now</h3>

                <h5><center><code>mapreduce.py</code></center></h5>
                <ol>
                    <li> <pre><code>get_filenames</code>Read in all text files</pre>
                    <li> <pre><code>get_data</code>Do text processing (strip/split)</pre>
                         <li> <pre><code>map_function</code>Count occurrences of words in each file</pre>
                         <li> <pre><code>reduce_function</code>Sums up occurrences across files</pre>
                         <li> <pre><code>show_items</code>Sorts results in descending order</pre>
                </ol>    

                </section>
                <section>
                <h3>The heart of map/reduce</h3>

                <pre>
                    
                            <code class="python" data-trim="">
                    def map(filename):
                    """
                    Takes a list of sentences,splits, 
                    and sums up words per sentence
                    :param filename: a single filename
                    """
                    result = {}
                    
                    for i in words:
                        try:
                            result[i] += 1
                        except KeyError:
                            result[i] = 1
                    return result   

                    def reduce_function(dict_list):
                    """
                    This function reduces a list of dicts by 
                    :param dict_list: List of words 
                    """
                    d = {}

                    for d in dict_list:
                            for k, v in d.items():
                                try:
                                    reduced_dict[k] += v
                                except KeyError:
                                    reduced_dict[k] = v
                    return d 
                        </code></pre><br>
                        <img  src="img/github.png"><a href="https://github.com/veekaybee/data-lake-code/blob/master/mapreduce.py">GitHub</a>
</section>    
                <section>
                <h3>The heart of map/reduce</h3>

                <pre>
                    
                            <code  data-trim="">
                map: {'valjean': 5, 'marius': 2, 'javert': 3}
                     {'valjean': 4, 'marius': 1, 'javert': 4}

                reduce: {'valjean': 9, 'marius':3, 'javert': 7}

                </code></pre>

                
            </section>

            <section> 
                <h2>How long does it take? 
                    <br> 3 GB = 5 minutes</h2>
                        <pre>
                        <code class="python" data-trim="">

word_list = []
files = get_filenames()
for filename in files: 
    mapr = map_function(filename)
    word_list.append(mapr)

mr = reduce_function(word_list)
show_items(mr)

vboykis$ time python mapreduce.py 
Size of files: 3.099123327061534 GB
Processing textfiles...
[1]+  Stopped  python mapreduce.py
real    5m14.605s
                </code></pre>

                </section>

                <section> <h2>How do we speed it up?<h2> 

                    üéâ Multiprocessing!üéâ 
                    <br>(and üêçPyPyüêç)

                    </section>

                    <section> <h3>What are threads and processes? </h3> 
                        (And what's a GIL?)

                        <img src="img/gil.png">
                        

                </section>


                    <section> <h3>How Python Utilizes Hardware</h3> 

                        <pre>
<code class="python" data-trim="">


vboykis$ python mapreduce.py
vboykis$ top
PID    COMMAND      %CPU  TIME     #TH       STATE
32057  Python       99.0  00:04.16 1/1    running
</code></pre>


                <img src="img/threadscores.png">
            </section>

            <section> <h2>Multiprocessing API</h2> 
                
                <div align="left">
                <p>
                <font size="6" align="left"><b>Process:</b> Each function call is a separate process (good for small programs)<br>
                <br><b>Pool:</b> Allows you to control number of processes, maintains order of returned results</font></p></div>
                <img src="img/processpool.png">

                </section>

            <section>

                
                <h3>Our Multiprocessing Code</h3>


 			<pre>
                 <code class="python" data-trim="">
                p.map(func, iterable[, chunksize])

                # parallel equivalent of map function
                # iterable is split into pieces of approx that size
                # submitted to process pool as separate tasks 
				</code>
                                </pre>	

                <pre>
                            <code class="python" data-trim="">

    files = get_filenames()
    pool= Pool(processes=5) 
    result_set = pool.map(map_function, files, chunksize=30) 
    show_items(reduce_function(result_set))

    PID    COMMAND      %CPU TIME     #TH    
    32984  Python       94.4 01:16.48 1/1    
    32983  Python       94.8 01:14.76 1/1   
    32982  Python       77.4 01:17.70 1/1   
    32981  Python       95.2 01:15.60 1/1   
    32980  Python       96.9 01:14.00 1/1    

                                </code>
                                    </pre><img align="middle" src="img/github.png"><a href="https://github.com/veekaybee/data-lake-code/blob/master/mapreduce.py">GitHub</a>
            </section>

    <section>
                <h3>Our Multiprocessing Code</h3>
                <br> 3 GB = 3 minutes
                <pre>

                     <code class="python" data-trim="">

                    pool= Pool(processes=5)
                    result_set = pool.map(map_function, get_filenames(), chunksize=30)

                    vboykis$ time mapreduce.py
                    ('Size of files:', 3.099123327061534, 'GB')
                    
                    real    2m56.571s  user    11m54.280s    0m29.430s
                    'valjean' - 790790
                    'marius' - 909909
                    'jean' - 1143142

                    </code>
                                    </pre>
					</section>

					 <section>

					<h3>Speeding it up with PyPy</h3>

                    "If you want your code to run faster, you should probably just use PyPy.‚Äù ‚Äî Guido"
						
                    
					Good for longer processes, mostly Python codebases.
<pre>
                     <code class="python" data-trim="">
                    vboykis$ time pypy mapreduce.py

                    real    2m10.799s  user    6m28.577s  sys    0m42.156s
                    ('Size of files:', 3.099123327061534, 'GB')

                    vboykis$ time pypy mapreduce.py
                    ('Size of files:', 6.195150626823306, 'GB')

                    real    4m23.917suser 12m6.691ssys  1m39.279s

                   

                                </code>
                                    </pre>

</section>




            <section>
                <h3>Recap</h3>

                <ul>
                    <li> You can process fairly large amounts of data(5-10 GB) locally 
                    <li> Structure your code for multiprocessing
                    <li> Try different Pool/Chunksize combinations 
                    <li> PyPy!
                    </ul> 

            </section>
                               


            <section>
                <h3>The cost of a system</h3>
                <img src="img/blowfish.png">
                <br><img align="left" src="img/cost.png"> In some cases, an optimized single node is better than unoptimized multiple nodes. <a href="https://pdfs.semanticscholar.org/6753/959eed800e9fad9e330daae43f81b7a48017.pdf"><br>
                Source</a>
            </section>


            <section>


                <h4>MapReduce with Spark locally: <br>less code, more overhead</h4>

                <div id="column1" style="float:left; width:50%;">
                <img src="img/threads.png"> 
                <a href="http://www.python-course.eu/threads.php"><br>
                <font size="-1">source</font></a> </div>

                <div id="column2" style="float:left; width:50%;">
                <img src="img/spark-local-architecture.png"> 
                <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html"><font size="-1">source</font></a></div>
                

                <pre>
                            <code class="python" data-trim=""> mbp-vboykis$ ./bin/spark-submit --master local[5]/spark_wc.py
                            </code>
                                    </pre>
            </section>
            <section>
                <h4>MapReduce with Spark</h4>
                <pre>
                            <code class="python" data-trim=""> 
                           

                    sc = SparkContext("local", "Les Mis Word Count") 

                    logFile = "/lesmiserables*.txt"

                    wordcounts = sc.textFile(logFile).map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \
                            .flatMap(lambda x: x.split()) \
                            .map(lambda x: (x, 1)) \
                            .reduceByKey(lambda x,y:x+y) \
                            .map(lambda x:(x[1],x[0])) \
                            .sortByKey(False) 
                    
                    print(wordcounts.take(10)) #print first 10 results 
                    sc.stop()
                                </code>
                                    </pre><img align="middle" src="img/github.png"><a href="https://github.com/veekaybee/data-lake-code/blob/master/spark_wordcount.py">GitHub</a>
            </section>

            <section>
                
                <h4>What big data can't give you</h4><img src="img/message-in-a-bottle.png"><br>
                <ul>
                    <li>Data integrity
                        <ul>
                            <li>(denormalized, naming conventions)</li>
                        </ul>
                    </li>
                    <li>SQL Query analyst speed</li>
                    <li>Traditional database guarantees (consistency)</li>
                </ul>

            </section>
            
            <section>
                <h4>What big data can give you</h4><img src="img/whale-breathing.png">
                <ul>
                    <li>Storage of unstructured data</li>
                    <li>Fault tolerance and availability</li>
                    <li>A centralized place across departments</li>
                    <li>Extremely heavy parallelized usage</li>
                    <li>Ability to programmatically work with data</li>
                </ul>
            </section>
            <section>
                <h3>Good use cases for Hadoop</h3><img src="img/elephant.png">
                <ul>
                    <li> A lot of data (more than 1 TB and growing)
                    <li>Unstructured data (images, video, <a href="https://twitter.com/denormalize/status/832442855683616768">metadata</a>) </li>
                    <li>Streaming data that needs to be stored quickly (logs) </li>
                    <li>Many researchers need access in parallel</li>
                    <li>You need to access and analyze ALL THE DATA </li>
                    <li>You have a dedicated team of people to work on it 
                        (@ least 1-2 dev, 1 admin, 1 analyst)</li><br>

                </ul>
            </section>

            



            <section>
                <h2>Thanks!/ Resources</h2>

                Vicki Boykis | @vboykis <br>

                <div align="center">

             


                <ul>Links:
                    <li><a href= "https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">Don't use Hadoop - your data isn't that big</a>
                    </li>
                    <li><a href= "https://github.com/aphyr/distsys-class/blob/master/README.markdown">An introduction to distributed systems</a>

                        <li><a href= "https://www.slideshare.net/databricks/spark-summit-east-2017-matei-zaharia-keynote-trends-for-big-data-and-apache-spark-in-2017">Multithreading and Python</a>
                            
                            

                    </li>
                </ul>

                   <p>Icons: made by <a href="http://www.freepik.com" title="Freepik">Freepik</a> <br>from <a href=http://www.flaticon.com" title="Flaticon">www.flaticon.com</a> <br>are licensed by <a href="http://creativecommons.org/licenses/by/3.0/"title="Creative Commons BY 3.0">CC 3.0 </a><br>
            </div>

            

            </section>
        </div>
    </div>
    <script src="lib/js/head.min.js">
    </script> 
    <script src="js/reveal.js">
    </script> 
    <script>
               // More info https://github.com/hakimel/reveal.js#configuration
               Reveal.initialize({
                   history: true,

                   transition: 'none',

                    slideNumber: true,

                   // More info https://github.com/hakimel/reveal.js#dependencies
                   dependencies: [
                       { src: 'plugin/markdown/marked.js' },
                       { src: 'plugin/markdown/markdown.js' },
                       { src: 'plugin/notes/notes.js', async: true },
                       { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
                   ]
               });
    </script>
</body>
</html>